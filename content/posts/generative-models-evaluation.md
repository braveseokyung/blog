+++
title = 'ìƒì„±ëª¨ë¸ í‰ê°€ì§€í‘œ IS, FID, LPIPS'
date = 2024-08-07T19:35:06+09:00
draft = false
authors = ["braveseokyung"]
description = 'ìƒì„±ëª¨ë¸ì˜ í‰ê°€ì§€í‘œ IS, FID, LPIPS ì— ëŒ€í•´ ì•Œì•„ë³´ì!'
categories = ["ë”¥ëŸ¬ë‹"]
series = ["Generative-Models"]
series_weight = 2
+++

<!--more-->

generative modelì€ stochastic randomnessê°€ ìˆê³ , qualityë¥¼ í‰ê°€í•˜ëŠ” ê¸°ì¤€ì´ ì• ë§¤í•´ì„œ evaluationì´ ì–´ë µë‹¤.

## Evaluation : Sharpness x Diversity

ë‘˜ ì¤‘ ë” ë‚˜ì€ generationì€?

{{<image src="evaluating-generative-models/Untitled.png">}}

- qualitative analysis
    - human evaluation, mechanical Turk : expensive, biased, poor reproducibility
- quantitative evaluation
    - IS, FID, MMD, LPIPS

## Inception Scores (IS)

- ëª©í‘œ: Sharpness & Diversity ë¥¼ ë§Œì¡±í•˜ëŠ” sampleì„ ì›í•¨
- assumption
    1. labelled datasetìœ¼ë¡œ í•™ìŠµëœ generative modelì˜ sample qualityë¥¼ í‰ê°€í•˜ëŠ” ìƒí™©
    2. for any point x, label yë¥¼ ì˜ predictí•˜ëŠ” good probabilistic `classifier` c(y|x)ê°€ ìˆë‹¤

### Inception Scores(Sharpness)

{{<image src="evaluating-generative-models/Untitled%201.png">}}

Sharpnessê°€ ë†’ë‹¤ : classifierê°€ generated imageì— ëŒ€í•´ predictionì„ í•  ë•Œ `confident`í•˜ë‹¤(classë¥¼ ë¶„ëª…í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤)

<aside>
ğŸ’¡ â†’ classifierì˜ predictive `distribution` c(y|x)ì˜ entropyê°€ ë‚®ë‹¤

</aside>

{{<image src="evaluating-generative-models/Untitled%202.png">}}

- xê°€ unconditionally generated ë˜ì—ˆë‹¤ëŠ” ê°€ì •
- expectation over all samples(fake images)

### Inception Scores(Diversity)

{{<image src="evaluating-generative-models/Untitled%203.png">}}

Diversityê°€ ë†’ë‹¤: generated sampleì˜ `predicted` classê°€ ë‹¤ì–‘í•˜ë‹¤

<aside>
ğŸ’¡ c(y) ê°€ high entropyë¥¼ ê°–ëŠ”ë‹¤

</aside>

- $c(y)=E_{x\sim p}[c(y|x)]$ : classifierì˜ marginal predictive destribution

{{<image src="evaluating-generative-models/Untitled%204.png">}}

### Inception Scores(Sharpness x Diversity)

$IS=D\times S$

- sharpnessì™€ diversity ë‘ criterionì„ í•©ì¹¨
- `higher` IS better quality
- classifierê°€ not availiableì´ë¼ë©´ í° datasetì— ëŒ€í•´ í•™ìŠµëœ e.g., Inception Net trained on ImageNet dataset ê°™ì€ ê±¸ ì‚¬ìš©
- limitation
    - model should be trained on labeled data(training data dependent)
    - classifierì˜ ì„±ëŠ¥ì— ì˜ì¡´(model-dependent)

### Inception Scores ì •ë¦¬

{{<image src="evaluating-generative-models/Untitled%205.png">}}

{{<image src="evaluating-generative-models/Untitled%206.png">}}

$$
IS=D\times S=\exp(\mathbb{E}_x KL(p(y|x)||p(y)))
$$

D*Së¥¼ ì •ë¦¬í•˜ë©´ conditionalê³¼ marginal probability ì‚¬ì´ì˜ KL divergenceì™€ ê°™ë‹¤

- KL Divergence:
    
    $$
    D_{KL}(P||Q)=\sum\limits_{x}P(x)log{\frac{P(x)}{Q(x)}}
    $$
    

<aside>
ğŸ’¡ Mutual Information vs Cross entropy vs KL Divergence

* Mutual Information : 
$I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$
$=\sum_{x,y}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$
$=D_{KL}(P(x,y)||P(x)P(y))\geq 0$

* Cross entropy:
$H(P;Q)=\sum\limits_{x\in A_x}P(x)log\frac{1}{Q(x)}$

* KL Divergence
$D_{KL}(P||Q)=\sum\limits_{x}P(x)log{\frac{P(x)}{Q(x)}}$

</aside>

## Frechet Inception Distance (FID)

- Inception scoreëŠ” $p_\theta$ ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ sampleë§Œ í•„ìš”
    - p_dataë¥¼ ì§ì ‘ì ìœ¼ë¡œ evaluateí•˜ì§€ ì•Šê³  classifierë¥¼ í†µí•´ implicití•˜ê²Œ take into accountí•œë‹¤
- Frechet Inception Distance(FID)ëŠ” $p_\theta$ ì™€ test datasetë¡œë¶€í„° sampleëœ data pointë“¤ì˜ feature representationì˜ similarityë¥¼ ì¸¡ì •(e.g., pretrained classifier ë¡œ ì–»ìŒ)

### Computing FID

generated sampleê³¼ test datasetì˜ feature representationì´ fittingë˜ëŠ” distributionì˜ distanceê°€ ì‘ìœ¼ë©´ ì¢‹ë‹¤

- $\mathcal{G}$  : generated sample
    
    $\mathcal{T}$ : test dataset
    
- $\mathcal{G},\mathcal{T}$ ì— ëŒ€í•´ feature representation $F_{\mathcal{G}}, F_{\mathcal{T}}$ ë¥¼ ì–»ìŒ (e.g., by prefinal layer of Inception Net)
- $F_{\mathcal{G}}, F_{\mathcal{T}}$ ì— ëŒ€í•´ multivariate Gaussianì„ fit
    - $(\mu_{\mathcal{G}}, \sum_{\mathcal{G}}),(\mu_{\mathcal{T}}, \sum_{\mathcal{T}})$ : mean, covariance of two Gaussian
- `FIDëŠ” ì´ ë‘ gaussian ì‚¬ì´ì˜ Wasserstein-2 distance`
    
    {{<image src="evaluating-generative-models/Untitled%207.png">}}
    
    - trace ì•ˆì˜ ë¶€ë¶„ì€ L2 normê³¼ ìœ ì‚¬
        
        {{<image src="evaluating-generative-models/Untitled%208.png">}}
        
    - frobenius norm
        
        {{<image src="evaluating-generative-models/Untitled%209.png">}}

        $A^H :$  conjugate transpose
        
- FIDê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ sample quality (ë‘ gaussian ì‚¬ì´ì˜ distanceê°€ ì‘ë‹¤ëŠ” ëœ»ì´ë‹ˆê¹Œ)

## LPIPS

{{<image src="evaluating-generative-models/Untitled%2010.png">}}

- $x,x_0$ ëŠ” generated sampleê³¼ test dataset
- featureì˜ ìœ ì‚¬ë„
- ëª¨ë“  layerì˜ ê°’ì„ sum
- $\hat y^l,\hat y_0^l\in \mathbb{R}^{H_l\times W_l\times C_l}$ : activation at layer l
- $w_l$ : scales/weights for channels in layer l
- ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ!
- fidelityë‚˜ diversity ì¸¡ë©´ë³´ë‹¤ëŠ” xì™€ x_0 ì‚¬ì´ì— ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ë³´ëŠ” ì§€í‘œ

### â­Â Image Space vs Feature Space

{{<image src="evaluating-generative-models/image-space-feature-space.png">}}

image space ì°¨ì›ì—ì„œ ëª¨ë¸ì˜ evaluationê³¼ ì‚¬ëŒì˜ evaluationì€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ ì´ë¯¸ì§€ì˜ ëª¨ë“  pixelì„ í•œ pixelì”© ì˜†ìœ¼ë¡œ ì´ë™í•œë‹¤ê³  í•´ë³´ì. ì‚¬ëŒì˜ ëˆˆìœ¼ë¡œëŠ” ì°¨ì´ë¥¼ êµ¬ë³„í•  ìˆ˜ ì—†ê² ì§€ë§Œ, ëª¨ë¸ì´ í‰ê°€í•  ë•ŒëŠ” ëª¨ë“  pixelê°’ì´ ë‹¤ë¥´ë¯€ë¡œ ì°¨ì´ê°€ ìˆë‹¤ê³  íŒë‹¨í•  ê²ƒì´ë‹¤. íŠ¹íˆ ëª¨ë“  pixelì— ëŒ€í•´ L2 lossë¥¼ êµ¬í•œë‹¤ê³  ì¹˜ë©´ ê·¸ ì°¨ì´ëŠ” ë” ì¦í­ë˜ê³  pixelì´ ë°€ë¦° ì´ë¯¸ì§€ê°€ ë” ë‚®ì€ qualityì˜ ì´ë¯¸ì§€ë³´ë‹¤ Lossê°€ í° ê²½ìš°ë„ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, image-spaceê°€ ì•„ë‹Œ feature-space ì¦‰, pixel levelì´ ì•„ë‹ˆë¼ feature levelì—ì„œ í‰ê°€í•˜ëŠ” ë°©ë²•ë¡ ì´ ë§ë‹¤.

## Reference
DATA303 Lecture note 15. Evaluating Generative Models